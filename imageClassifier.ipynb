{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"gpuClass":"standard","accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport random\nimport os, numpy as np\nimport torch\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\n\nfrom skimage.transform import resize\nfrom scipy.sparse import csr_matrix\nfrom PIL import Image\nimport xml.etree.ElementTree as ET\n\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nVOC_CLASSES = (\n    \"__background__\",\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",\n    \"chair\",\n    \"cow\",\n    \"diningtable\",\n    \"dog\",\n    \"horse\",\n    \"motorbike\",\n    \"person\",\n    \"pottedplant\",\n    \"sheep\",\n    \"sofa\",\n    \"train\",\n    \"tvmonitor\",\n)\n\n\nclass VocDataset(data.Dataset):\n    def __init__(self, data_path, dataset_split, transform, random_crops=0):\n        self.data_path = data_path\n        self.transform = transform\n        self.random_crops = random_crops\n        self.dataset_split = dataset_split\n\n        self.__init_classes()\n        (\n            self.names,\n            self.labels,\n            self.box_indices,\n            self.label_order,\n        ) = self.__dataset_info()\n\n    def __getitem__(self, index):\n        # CHANGED\n        #         x = imread(self.data_path + '/JPEGImages/' + self.names[index] + '.jpg', mode='RGB')\n        #         x = Image.fromarray(x)\n        x = Image.open(self.data_path + \"/JPEGImages/\" + self.names[index] + \".jpg\")\n\n        scale = np.random.rand() * 2 + 0.25\n        w = int(x.size[0] * scale)\n        h = int(x.size[1] * scale)\n        if min(w, h) < 227:\n            scale = 227 / min(w, h)\n            w = int(x.size[0] * scale)\n            h = int(x.size[1] * scale)\n\n        if self.random_crops == 0:\n            x = self.transform(x)\n        else:\n            crops = []\n            for i in range(self.random_crops):\n                crops.append(self.transform(x))\n            x = torch.stack(crops)\n\n        y = self.labels[index]\n        return x, y\n\n    def __len__(self):\n        return len(self.names)\n\n    def __init_classes(self):\n        self.classes = VOC_CLASSES\n        self.num_classes = len(self.classes)\n        self.class_to_ind = dict(zip(self.classes, range(self.num_classes)))\n\n    def __dataset_info(self):\n        with open(\n            self.data_path + \"/ImageSets/Main/\" + self.dataset_split + \".txt\"\n        ) as f:\n            annotations = f.readlines()\n\n        annotations = [n[:-1] for n in annotations]\n        box_indices = []\n        names = []\n        labels = []\n        label_order = []\n        for af in annotations:\n            if len(af) != 6:\n                continue\n            filename = os.path.join(self.data_path, \"Annotations\", af)\n            tree = ET.parse(filename + \".xml\")\n            objs = tree.findall(\"object\")\n            num_objs = len(objs)\n\n            boxes = np.zeros((num_objs, 4), dtype=np.int32)\n            boxes_cl = np.zeros((num_objs), dtype=np.int32)\n            boxes_cla = []\n            temp_label = []\n            for ix, obj in enumerate(objs):\n                bbox = obj.find(\"bndbox\")\n                # Make pixel indexes 0-based\n                x1 = float(bbox.find(\"xmin\").text) - 1\n                y1 = float(bbox.find(\"ymin\").text) - 1\n                x2 = float(bbox.find(\"xmax\").text) - 1\n                y2 = float(bbox.find(\"ymax\").text) - 1\n\n                cls = self.class_to_ind[obj.find(\"name\").text.lower().strip()]\n                boxes[ix, :] = [x1, y1, x2, y2]\n                boxes_cl[ix] = cls\n                boxes_cla.append(boxes[ix, :])\n                temp_label.append(cls)\n\n            lbl = np.zeros(self.num_classes)\n            lbl[boxes_cl] = 1\n            labels.append(lbl)\n            names.append(af)\n            box_indices.append(boxes_cla)\n            label_order.append(temp_label)\n\n        return (\n            np.array(names),\n            np.array(labels).astype(np.float32),\n            np.array(box_indices),\n            label_order,\n        )","metadata":{"id":"TCdQ65gecPOR","execution":{"iopub.status.busy":"2023-03-01T20:21:22.518431Z","iopub.execute_input":"2023-03-01T20:21:22.518972Z","iopub.status.idle":"2023-03-01T20:21:22.653009Z","shell.execute_reply.started":"2023-03-01T20:21:22.518929Z","shell.execute_reply":"2023-03-01T20:21:22.651955Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import os\nimport csv\nimport numpy as np\n\n\ndef write_csv(file_path, y_list):\n    solution_rows = [(\"id\", \"category\")] + [(i, 1 - y) for (i, y) in enumerate(y_list)]\n    with open(file_path, \"w\") as f:\n        writer = csv.writer(f)\n        writer.writerows(solution_rows)\n\n\ndef output_submission_csv(output_file_path, y_test):\n    write_csv(output_file_path, y_test)","metadata":{"id":"UXMnB6HpcRpx","execution":{"iopub.status.busy":"2023-03-01T20:21:27.481053Z","iopub.execute_input":"2023-03-01T20:21:27.481686Z","iopub.status.idle":"2023-03-01T20:21:27.527801Z","shell.execute_reply.started":"2023-03-01T20:21:27.481645Z","shell.execute_reply":"2023-03-01T20:21:27.526533Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Assignment 2 Part 2: Developing Your Own Classifier","metadata":{"id":"iQgM-yy9ZCD9"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\n\nfrom torchvision import transforms\nfrom sklearn.metrics import average_precision_score\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\n# from kaggle_submission import output_submission_csv\n# from classifier import SimpleClassifier, Classifier#, AlexNet\n# from voc_dataloader import VocDataset, VOC_CLASSES\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2","metadata":{"id":"6qETMEBCZCEA","execution":{"iopub.status.busy":"2023-03-01T20:21:29.676790Z","iopub.execute_input":"2023-03-01T20:21:29.677552Z","iopub.status.idle":"2023-03-01T20:21:29.729304Z","shell.execute_reply.started":"2023-03-01T20:21:29.677515Z","shell.execute_reply":"2023-03-01T20:21:29.727790Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Part 2: Design your own network\n\nIn this notebook, your task is to create and train your own model for multi-label classification on VOC Pascal.\n\n## What to do\n1. You will make change on network architecture in ```classifier.py```.\n2. You may also want to change other hyperparameters to assist your training to get a better performances. Hints will be given in the below instructions.\n\n## What to submit\nCheck the submission template for details what to submit. ","metadata":{"id":"liR86zO4ZCEC"}},{"cell_type":"code","source":"def train_classifier(train_loader, classifier, criterion, optimizer):\n    classifier.train()\n    loss_ = 0.0\n    losses = []\n    for i, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logits = classifier(images)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss)\n    return torch.stack(losses).mean().item()","metadata":{"id":"o42rdQQ3ZCED","execution":{"iopub.status.busy":"2023-03-01T20:21:31.169552Z","iopub.execute_input":"2023-03-01T20:21:31.170289Z","iopub.status.idle":"2023-03-01T20:21:31.216085Z","shell.execute_reply.started":"2023-03-01T20:21:31.170248Z","shell.execute_reply":"2023-03-01T20:21:31.214856Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\ndef test_classifier(test_loader, classifier, criterion, print_ind_classes=True, print_total=True):\n    classifier.eval()\n    losses = []\n    with torch.no_grad():\n        y_true = np.zeros((0,21))\n        y_score = np.zeros((0,21))\n        for i, (images, labels) in enumerate(test_loader):\n            images, labels = images.to(device), labels.to(device)\n            logits = classifier(images)\n            y_true = np.concatenate((y_true, labels.cpu().numpy()), axis=0)\n            y_score = np.concatenate((y_score, logits.cpu().numpy()), axis=0)\n            loss = criterion(logits, labels)\n            losses.append(loss.item())\n        aps = []\n        # ignore first class which is background\n        for i in range(1, y_true.shape[1]):\n            ap = average_precision_score(y_true[:, i], y_score[:, i])\n            if print_ind_classes:\n                print('-------  Class: {:<12}     AP: {:>8.4f}  -------'.format(VOC_CLASSES[i], ap))\n            aps.append(ap)\n        \n        mAP = np.mean(aps)\n        test_loss = np.mean(losses)\n        if print_total:\n            print('mAP: {0:.4f}'.format(mAP))\n            print('Avg loss: {}'.format(test_loss))\n        \n    return mAP, test_loss, aps","metadata":{"id":"jDUmJSl_ZCED","execution":{"iopub.status.busy":"2023-03-01T20:21:31.972362Z","iopub.execute_input":"2023-03-01T20:21:31.972782Z","iopub.status.idle":"2023-03-01T20:21:32.022834Z","shell.execute_reply.started":"2023-03-01T20:21:31.972731Z","shell.execute_reply":"2023-03-01T20:21:32.021653Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\ndef plot_losses(train, val, test_frequency, num_epochs):\n    plt.plot(train, label=\"train\")\n    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n    plt.plot(indices, val, label=\"val\")\n    plt.title(\"Loss Plot\")\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.legend()\n    plt.show()\n    \ndef plot_mAP(train, val, test_frequency, num_epochs):\n    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n    plt.plot(indices, train, label=\"train\")\n    plt.plot(indices, val, label=\"val\")\n    plt.title(\"mAP Plot\")\n    plt.ylabel(\"mAP\")\n    plt.xlabel(\"Epoch\")\n    plt.legend()\n    plt.show()\n    ","metadata":{"id":"2Pry04gKZCEE","execution":{"iopub.status.busy":"2023-03-01T20:21:32.694548Z","iopub.execute_input":"2023-03-01T20:21:32.695256Z","iopub.status.idle":"2023-03-01T20:21:32.742005Z","shell.execute_reply.started":"2023-03-01T20:21:32.695213Z","shell.execute_reply":"2023-03-01T20:21:32.740874Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\ndef train(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency=5):\n    train_losses = []\n    train_mAPs = []\n    val_losses = []\n    val_mAPs = []\n    \n    for epoch in range(1,num_epochs+1):\n        print(\"Starting epoch number \" + str(epoch))\n        train_loss = train_classifier(train_loader, classifier, criterion, optimizer)\n\n        torch.save(classifier.state_dict(), '/kaggle/working/cnn_weights.pth')\n        \n        train_losses.append(train_loss)\n        print(\"Loss for Training on Epoch \" +str(epoch) + \" is \"+ str(train_loss))\n        if(epoch%test_frequency==0 or epoch==1):\n            mAP_train, _, _ = test_classifier(train_loader, classifier, criterion, False, False)\n            train_mAPs.append(mAP_train)\n            mAP_val, val_loss, _ = test_classifier(val_loader, classifier, criterion)\n            print('Evaluating classifier')\n            print(\"Mean Precision Score for Testing on Epoch \" +str(epoch) + \" is \"+ str(mAP_val))\n            val_losses.append(val_loss)\n            val_mAPs.append(mAP_val)\n    \n    return classifier, train_losses, val_losses, train_mAPs, val_mAPs","metadata":{"id":"Dxh3p0bwZCEE","execution":{"iopub.status.busy":"2023-03-01T20:21:33.277458Z","iopub.execute_input":"2023-03-01T20:21:33.277959Z","iopub.status.idle":"2023-03-01T20:21:33.326943Z","shell.execute_reply.started":"2023-03-01T20:21:33.277923Z","shell.execute_reply":"2023-03-01T20:21:33.325661Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Developing Your Own Model","metadata":{"id":"Yt2IJF1IZCEF"}},{"cell_type":"markdown","source":"### Goal\nTo meet the benchmark for this assignment you will need to improve the network. Note you should have noticed pretrained Alenxt performs really well, but training Alexnet from scratch performs much worse. We hope you can design a better architecture over both the simple classifier and AlexNet to train from scratch.\n\n### How to start\nYou may take inspiration from other published architectures and architectures discussed in lecture. However, you are NOT allowed to use predefined models (e.g. models from torchvision) or use pretrained weights. Training must be done from scratch with your own custom model.\n\n#### Some hints\nThere are a variety of different approaches you should try to improve performance from the simple classifier:\n\n* Network architecture changes\n    * Number of layers: try adding layers to make your network deeper\n    * Batch normalization: adding batch norm between layers will likely give you a significant performance increase\n    * Residual connections: as you increase the depth of your network, you will find that having residual connections like those in ResNet architectures will be helpful\n* Optimizer: Instead of plain SGD, you may want to add a learning rate schedule, add momentum, or use one of the other optimizers you have learned about like Adam. Check the `torch.optim` package for other optimizers\n* Data augmentation: You should use the `torchvision.transforms` module to try adding random resized crops and horizontal flips of the input data. Check `transforms.RandomResizedCrop` and `transforms.RandomHorizontalFlip` for this. Feel free to apply more [transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) for data augmentation which can lead to better performance. \n* Epochs: Once you have found a generally good hyperparameter setting try training for more epochs\n* Loss function: You might want to add weighting to the `MultiLabelSoftMarginLoss` for classes that are less well represented or experiment with a different loss function\n\n\n\n#### Note\nWe will soon be providing some initial expectations of mAP values as a function of epoch so you can get an early idea whether your implementation works without waiting a long time for training to converge.\n\n### What to submit \nSubmit your best model to Kaggle and save all plots for the writeup.\n","metadata":{"id":"pyfLN8G_ZCEF"}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std= [0.229, 0.224, 0.225])\n\ntrain_transform = transforms.Compose([\n            transforms.Resize(227),\n            transforms.CenterCrop(227),\n            transforms.ToTensor(),\n            normalize\n        ])\n\ntest_transform = transforms.Compose([\n            transforms.Resize(227),\n            transforms.CenterCrop(227),\n            transforms.ToTensor(),\n            normalize,\n        ])\n\nds_train = VocDataset('/kaggle/input/pascal2/VOCdevkit_2007/VOC2007','train',train_transform)\nds_val = VocDataset('/kaggle/input/pascal2/VOCdevkit_2007/VOC2007','val',test_transform)\nds_test = VocDataset('/kaggle/input/pascal2/VOCdevkit_2007/VOC2007test','test', test_transform)\n","metadata":{"id":"vsF99WVNZCEG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a66b0aa6-2c9c-4e3b-ed58-ba9da973ba0c","execution":{"iopub.status.busy":"2023-03-01T20:21:35.015478Z","iopub.execute_input":"2023-03-01T20:21:35.016137Z","iopub.status.idle":"2023-03-01T20:21:41.228105Z","shell.execute_reply.started":"2023-03-01T20:21:35.016097Z","shell.execute_reply":"2023-03-01T20:21:41.226897Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:137: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom random import *\n\n# Define the transformations\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(randint(10, 350)),\n    transforms.RandomPerspective(distortion_scale=0.3, p=1.0),\n    transforms.RandomResizedCrop(227),\n    transforms.RandomVerticalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load the original dataset\ntempds_train = VocDataset('/kaggle/input/pascal2/VOCdevkit_2007/VOC2007','train',train_transform)\n\n# Apply the transformations to the original dataset to create a new dataset\nds_train = torch.utils.data.ConcatDataset([\n    tempds_train,\n    VocDataset('/kaggle/input/pascal2/VOCdevkit_2007/VOC2007','train',train_transform),\n    VocDataset('/kaggle/input/pascal2/VOCdevkit_2007/VOC2007','train',train_transform),\n    VocDataset('/kaggle/input/pascal2/VOCdevkit_2007/VOC2007','train',train_transform),\n    VocDataset('/kaggle/input/pascal2/VOCdevkit_2007/VOC2007','train',train_transform),\n    VocDataset('/kaggle/input/pascal2/VOCdevkit_2007/VOC2007','train',train_transform),\n    VocDataset('/kaggle/input/pascal2/VOCdevkit_2007/VOC2007','train',train_transform),\n    \n])\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqCuj4sKA1TK","outputId":"fc7e7070-3ca3-4bc4-853f-9a950e49f106","execution":{"iopub.status.busy":"2023-03-01T20:21:41.232087Z","iopub.execute_input":"2023-03-01T20:21:41.232903Z","iopub.status.idle":"2023-03-01T20:21:52.590314Z","shell.execute_reply.started":"2023-03-01T20:21:41.232872Z","shell.execute_reply":"2023-03-01T20:21:52.589123Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:137: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","output_type":"stream"}]},{"cell_type":"code","source":"len(ds_train)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMFRJF_JBTlc","outputId":"eeb24049-4526-4b45-8af6-0a61941365a6","execution":{"iopub.status.busy":"2023-03-01T20:21:52.593783Z","iopub.execute_input":"2023-03-01T20:21:52.594115Z","iopub.status.idle":"2023-03-01T20:21:52.639118Z","shell.execute_reply.started":"2023-03-01T20:21:52.594085Z","shell.execute_reply":"2023-03-01T20:21:52.637804Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"17507"},"metadata":{}}]},{"cell_type":"code","source":"num_epochs = 10\ntest_frequency = 5\nbatch_size = 64\n\ntrain_loader = torch.utils.data.DataLoader(dataset=ds_train,\n                                               batch_size=batch_size, \n                                               shuffle=True,\n                                               num_workers=1)\n\nval_loader = torch.utils.data.DataLoader(dataset=ds_val,\n                                               batch_size=batch_size, \n                                               shuffle=True,\n                                               num_workers=1)\n\ntest_loader = torch.utils.data.DataLoader(dataset=ds_test,\n                                               batch_size=batch_size, \n                                               shuffle=False,\n                                               num_workers=1)","metadata":{"id":"6I2mg2ygZCEG","execution":{"iopub.status.busy":"2023-03-01T20:21:52.641986Z","iopub.execute_input":"2023-03-01T20:21:52.642474Z","iopub.status.idle":"2023-03-01T20:21:52.687348Z","shell.execute_reply.started":"2023-03-01T20:21:52.642429Z","shell.execute_reply":"2023-03-01T20:21:52.686154Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\nNUM_CLASSES = 21\nimport torch.nn.init as init\n\nimport torch.nn as nn\n\nclass ResBlock(nn.Module):\n    \n    def __init__(self, input_filters, output_filters, stride=1):\n        super(ResBlock, self).__init__()\n    \n        if stride != 1 or input_filters != output_filters:\n            self.reslink = nn.Sequential(nn.Conv2d(input_filters, output_filters, 1, stride, bias=False),nn.BatchNorm2d(output_filters))\n        else:\n            self.reslink = nn.Sequential()\n\n        self.conv1 = nn.Conv2d(input_filters, output_filters, 3, stride, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(output_filters)\n\n        self.conv2 = nn.Conv2d(output_filters, output_filters, 3, 1, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(output_filters)\n\n        self.relu = nn.ReLU()\n\n\n\n    def forward(self, x):\n        \n        out = self.relu(self.bn1(self.conv1(x)))\n        \n        out = self.bn2(self.conv2(out))\n        \n        out += self.reslink(x)\n        out = self.relu(out)\n        \n        return out\n\n\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, 3, 2, 1)\n\n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool = nn.MaxPool2d(2,2)\n\n        self.avgpool2 = nn.AvgPool2d((2, 2))\n        \n        self.relu = nn.ReLU(inplace=True)\n        self.resblock1 = ResBlock(64, 128)\n        self.resblock2 = ResBlock(128, 256,2) \n        self.resblock3 = ResBlock(256, 512,2)\n        self.resblock4 = ResBlock(512, 1024,2)\n        \n        self.fc1 = nn.Linear(16384, 8192)\n        self.fc2 = nn.Linear(8192, 1000)\n        self.fc3 = nn.Linear(1000,21)\n\n        self.dropout = nn.Dropout(0.5)\n\n        \n    def forward(self, x):\n\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.maxpool(out)\n        \n\n        out = self.resblock1(out)\n        out = self.resblock2(out)\n        out = self.resblock3(out)\n        out = self.resblock4(out) \n        \n        out = self.avgpool2(out)\n\n        out = out.view(out.size(0), -1)\n\n        out = self.fc1(out)\n        out = self.dropout(out)\n        out = self.fc2(out) \n        out = self.dropout(out)\n        out = self.fc3(out)    \n        \n        return out\n","metadata":{"id":"QF3qNvSbSzU0","execution":{"iopub.status.busy":"2023-03-01T20:23:20.947650Z","iopub.execute_input":"2023-03-01T20:23:20.948262Z","iopub.status.idle":"2023-03-01T20:23:21.007227Z","shell.execute_reply.started":"2023-03-01T20:23:20.948216Z","shell.execute_reply":"2023-03-01T20:23:21.006033Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\nnum_epochs = 30\ntest_frequency = 5\nbatch_size = 64\n\nclassifier = Classifier().to(device)\n\n# classifier.load_state_dict(torch.load('cnn_weights.pth'))\n\ncriterion = nn.MultiLabelSoftMarginLoss()\noptimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n\nclassifier, train_losses, val_losses, train_mAPs, val_mAPs = train(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXBtDsfoSzR_","outputId":"ebaa5ad3-78aa-4965-fd3d-cf1c25fbc66e","execution":{"iopub.status.busy":"2023-03-01T20:23:21.408180Z","iopub.execute_input":"2023-03-01T20:23:21.408540Z","iopub.status.idle":"2023-03-02T00:15:56.519952Z","shell.execute_reply.started":"2023-03-01T20:23:21.408507Z","shell.execute_reply":"2023-03-02T00:15:56.515624Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Starting epoch number 1\nLoss for Training on Epoch 1 is 0.24324510991573334\n-------  Class: aeroplane        AP:   0.3811  -------\n-------  Class: bicycle          AP:   0.1498  -------\n-------  Class: bird             AP:   0.1353  -------\n-------  Class: boat             AP:   0.1666  -------\n-------  Class: bottle           AP:   0.0862  -------\n-------  Class: bus              AP:   0.0696  -------\n-------  Class: car              AP:   0.2741  -------\n-------  Class: cat              AP:   0.2250  -------\n-------  Class: chair            AP:   0.2393  -------\n-------  Class: cow              AP:   0.0797  -------\n-------  Class: diningtable      AP:   0.1743  -------\n-------  Class: dog              AP:   0.1550  -------\n-------  Class: horse            AP:   0.1552  -------\n-------  Class: motorbike        AP:   0.1420  -------\n-------  Class: person           AP:   0.4995  -------\n-------  Class: pottedplant      AP:   0.1039  -------\n-------  Class: sheep            AP:   0.0888  -------\n-------  Class: sofa             AP:   0.1787  -------\n-------  Class: train            AP:   0.1811  -------\n-------  Class: tvmonitor        AP:   0.0826  -------\nmAP: 0.1784\nAvg loss: 0.2220006465911865\nEvaluating classifier\nMean Precision Score for Testing on Epoch 1 is 0.17837930083907\nStarting epoch number 2\nLoss for Training on Epoch 2 is 0.2228534072637558\nStarting epoch number 3\nLoss for Training on Epoch 3 is 0.21668580174446106\nStarting epoch number 4\nLoss for Training on Epoch 4 is 0.21187685430049896\nStarting epoch number 5\nLoss for Training on Epoch 5 is 0.2081834226846695\n-------  Class: aeroplane        AP:   0.4673  -------\n-------  Class: bicycle          AP:   0.1801  -------\n-------  Class: bird             AP:   0.1668  -------\n-------  Class: boat             AP:   0.3010  -------\n-------  Class: bottle           AP:   0.1095  -------\n-------  Class: bus              AP:   0.0991  -------\n-------  Class: car              AP:   0.3531  -------\n-------  Class: cat              AP:   0.2622  -------\n-------  Class: chair            AP:   0.3050  -------\n-------  Class: cow              AP:   0.1015  -------\n-------  Class: diningtable      AP:   0.2689  -------\n-------  Class: dog              AP:   0.2320  -------\n-------  Class: horse            AP:   0.3593  -------\n-------  Class: motorbike        AP:   0.3299  -------\n-------  Class: person           AP:   0.6365  -------\n-------  Class: pottedplant      AP:   0.1304  -------\n-------  Class: sheep            AP:   0.1700  -------\n-------  Class: sofa             AP:   0.1862  -------\n-------  Class: train            AP:   0.2613  -------\n-------  Class: tvmonitor        AP:   0.1499  -------\nmAP: 0.2535\nAvg loss: 0.21181963235139847\nEvaluating classifier\nMean Precision Score for Testing on Epoch 5 is 0.253498384052832\nStarting epoch number 6\nLoss for Training on Epoch 6 is 0.20430980622768402\nStarting epoch number 7\nLoss for Training on Epoch 7 is 0.2012980580329895\nStarting epoch number 8\nLoss for Training on Epoch 8 is 0.19802480936050415\nStarting epoch number 9\nLoss for Training on Epoch 9 is 0.19548720121383667\nStarting epoch number 10\nLoss for Training on Epoch 10 is 0.19362108409404755\n-------  Class: aeroplane        AP:   0.5006  -------\n-------  Class: bicycle          AP:   0.2423  -------\n-------  Class: bird             AP:   0.1745  -------\n-------  Class: boat             AP:   0.3234  -------\n-------  Class: bottle           AP:   0.1297  -------\n-------  Class: bus              AP:   0.1641  -------\n-------  Class: car              AP:   0.4031  -------\n-------  Class: cat              AP:   0.3676  -------\n-------  Class: chair            AP:   0.3296  -------\n-------  Class: cow              AP:   0.1394  -------\n-------  Class: diningtable      AP:   0.2963  -------\n-------  Class: dog              AP:   0.2770  -------\n-------  Class: horse            AP:   0.4478  -------\n-------  Class: motorbike        AP:   0.3917  -------\n-------  Class: person           AP:   0.6746  -------\n-------  Class: pottedplant      AP:   0.1439  -------\n-------  Class: sheep            AP:   0.1935  -------\n-------  Class: sofa             AP:   0.2281  -------\n-------  Class: train            AP:   0.3417  -------\n-------  Class: tvmonitor        AP:   0.1989  -------\nmAP: 0.2984\nAvg loss: 0.20670495145022869\nEvaluating classifier\nMean Precision Score for Testing on Epoch 10 is 0.2983859395254291\nStarting epoch number 11\nLoss for Training on Epoch 11 is 0.19131016731262207\nStarting epoch number 12\nLoss for Training on Epoch 12 is 0.18844375014305115\nStarting epoch number 13\nLoss for Training on Epoch 13 is 0.18627798557281494\nStarting epoch number 14\nLoss for Training on Epoch 14 is 0.18297646939754486\nStarting epoch number 15\nLoss for Training on Epoch 15 is 0.18130794167518616\n-------  Class: aeroplane        AP:   0.6145  -------\n-------  Class: bicycle          AP:   0.3392  -------\n-------  Class: bird             AP:   0.2252  -------\n-------  Class: boat             AP:   0.3365  -------\n-------  Class: bottle           AP:   0.1403  -------\n-------  Class: bus              AP:   0.2048  -------\n-------  Class: car              AP:   0.5331  -------\n-------  Class: cat              AP:   0.3373  -------\n-------  Class: chair            AP:   0.3817  -------\n-------  Class: cow              AP:   0.2179  -------\n-------  Class: diningtable      AP:   0.3824  -------\n-------  Class: dog              AP:   0.2908  -------\n-------  Class: horse            AP:   0.4820  -------\n-------  Class: motorbike        AP:   0.3359  -------\n-------  Class: person           AP:   0.6880  -------\n-------  Class: pottedplant      AP:   0.1985  -------\n-------  Class: sheep            AP:   0.2413  -------\n-------  Class: sofa             AP:   0.2674  -------\n-------  Class: train            AP:   0.4402  -------\n-------  Class: tvmonitor        AP:   0.2952  -------\nmAP: 0.3476\nAvg loss: 0.19110760055482387\nEvaluating classifier\nMean Precision Score for Testing on Epoch 15 is 0.34761144341235195\nStarting epoch number 16\nLoss for Training on Epoch 16 is 0.17916877567768097\nStarting epoch number 17\nLoss for Training on Epoch 17 is 0.17661547660827637\nStarting epoch number 18\nLoss for Training on Epoch 18 is 0.1736576110124588\nStarting epoch number 19\nLoss for Training on Epoch 19 is 0.17216061055660248\nStarting epoch number 20\nLoss for Training on Epoch 20 is 0.16975411772727966\n-------  Class: aeroplane        AP:   0.6155  -------\n-------  Class: bicycle          AP:   0.3168  -------\n-------  Class: bird             AP:   0.2989  -------\n-------  Class: boat             AP:   0.3740  -------\n-------  Class: bottle           AP:   0.1531  -------\n-------  Class: bus              AP:   0.1732  -------\n-------  Class: car              AP:   0.5462  -------\n-------  Class: cat              AP:   0.3958  -------\n-------  Class: chair            AP:   0.3933  -------\n-------  Class: cow              AP:   0.2166  -------\n-------  Class: diningtable      AP:   0.4245  -------\n-------  Class: dog              AP:   0.2910  -------\n-------  Class: horse            AP:   0.4657  -------\n-------  Class: motorbike        AP:   0.3773  -------\n-------  Class: person           AP:   0.7249  -------\n-------  Class: pottedplant      AP:   0.1866  -------\n-------  Class: sheep            AP:   0.1984  -------\n-------  Class: sofa             AP:   0.2536  -------\n-------  Class: train            AP:   0.4387  -------\n-------  Class: tvmonitor        AP:   0.2501  -------\nmAP: 0.3547\nAvg loss: 0.19773662015795707\nEvaluating classifier\nMean Precision Score for Testing on Epoch 20 is 0.35471252664144354\nStarting epoch number 21\nLoss for Training on Epoch 21 is 0.16739657521247864\nStarting epoch number 22\nLoss for Training on Epoch 22 is 0.1658204048871994\nStarting epoch number 23\nLoss for Training on Epoch 23 is 0.1621241271495819\nStarting epoch number 24\nLoss for Training on Epoch 24 is 0.1597500741481781\nStarting epoch number 25\nLoss for Training on Epoch 25 is 0.1578563153743744\n-------  Class: aeroplane        AP:   0.5464  -------\n-------  Class: bicycle          AP:   0.3619  -------\n-------  Class: bird             AP:   0.3300  -------\n-------  Class: boat             AP:   0.3350  -------\n-------  Class: bottle           AP:   0.1510  -------\n-------  Class: bus              AP:   0.1621  -------\n-------  Class: car              AP:   0.5645  -------\n-------  Class: cat              AP:   0.3954  -------\n-------  Class: chair            AP:   0.4091  -------\n-------  Class: cow              AP:   0.1568  -------\n-------  Class: diningtable      AP:   0.3866  -------\n-------  Class: dog              AP:   0.2879  -------\n-------  Class: horse            AP:   0.4886  -------\n-------  Class: motorbike        AP:   0.4738  -------\n-------  Class: person           AP:   0.7541  -------\n-------  Class: pottedplant      AP:   0.1950  -------\n-------  Class: sheep            AP:   0.2251  -------\n-------  Class: sofa             AP:   0.2475  -------\n-------  Class: train            AP:   0.5538  -------\n-------  Class: tvmonitor        AP:   0.2718  -------\nmAP: 0.3648\nAvg loss: 0.1974387727677822\nEvaluating classifier\nMean Precision Score for Testing on Epoch 25 is 0.36482497047239926\nStarting epoch number 26\nLoss for Training on Epoch 26 is 0.15387336909770966\nStarting epoch number 27\nLoss for Training on Epoch 27 is 0.1509864628314972\nStarting epoch number 28\nLoss for Training on Epoch 28 is 0.14824609458446503\nStarting epoch number 29\nLoss for Training on Epoch 29 is 0.14654017984867096\nStarting epoch number 30\nLoss for Training on Epoch 30 is 0.14370259642601013\n-------  Class: aeroplane        AP:   0.6284  -------\n-------  Class: bicycle          AP:   0.3510  -------\n-------  Class: bird             AP:   0.3101  -------\n-------  Class: boat             AP:   0.3416  -------\n-------  Class: bottle           AP:   0.1559  -------\n-------  Class: bus              AP:   0.1593  -------\n-------  Class: car              AP:   0.5238  -------\n-------  Class: cat              AP:   0.3923  -------\n-------  Class: chair            AP:   0.3817  -------\n-------  Class: cow              AP:   0.1570  -------\n-------  Class: diningtable      AP:   0.3945  -------\n-------  Class: dog              AP:   0.2740  -------\n-------  Class: horse            AP:   0.5134  -------\n-------  Class: motorbike        AP:   0.4851  -------\n-------  Class: person           AP:   0.7629  -------\n-------  Class: pottedplant      AP:   0.2010  -------\n-------  Class: sheep            AP:   0.2518  -------\n-------  Class: sofa             AP:   0.2472  -------\n-------  Class: train            AP:   0.4057  -------\n-------  Class: tvmonitor        AP:   0.3167  -------\nmAP: 0.3627\nAvg loss: 0.21156557276844978\nEvaluating classifier\nMean Precision Score for Testing on Epoch 30 is 0.3626777703155669\nStarting epoch number 31\nLoss for Training on Epoch 31 is 0.14059077203273773\nStarting epoch number 32\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1825/521932273.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mAPs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mAPs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_1825/3286934609.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting epoch number \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/kaggle/working/cnn_weights.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_1825/593381809.py\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(train_loader, classifier, criterion, optimizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"plot_losses(train_losses, val_losses, test_frequency, num_epochs)\nplot_mAP(train_mAPs, val_mAPs, test_frequency, num_epochs)","metadata":{"id":"IkZbJuSfBtrz","execution":{"iopub.status.busy":"2023-03-02T00:25:55.012142Z","iopub.execute_input":"2023-03-02T00:25:55.012585Z","iopub.status.idle":"2023-03-02T00:25:55.077013Z","shell.execute_reply.started":"2023-03-02T00:25:55.012548Z","shell.execute_reply":"2023-03-02T00:25:55.075163Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1825/1328662703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_mAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mAPs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mAPs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"],"ename":"NameError","evalue":"name 'train_losses' is not defined","output_type":"error"}]},{"cell_type":"code","source":"\nmAP_test, test_loss, test_aps = test_classifier(test_loader, classifier, criterion)\nprint(mAP_test)\n                                       \noutput_submission_csv('my_solution.csv', test_aps)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T00:15:56.524653Z","iopub.status.idle":"2023-03-02T00:15:56.531310Z","shell.execute_reply.started":"2023-03-02T00:15:56.531031Z","shell.execute_reply":"2023-03-02T00:15:56.531059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(classifier.state_dict(), '/content/cnn_weights.pth')\n\n\nclassifier2 = Classifier().to(device)\nclassifier2.load_state_dict(torch.load('cnn_weights.pth'))\nprint(classifier2)","metadata":{"id":"qhJ-mimboCnN"},"execution_count":null,"outputs":[]}]}
